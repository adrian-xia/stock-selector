"""å®šæ—¶ä»»åŠ¡å®šä¹‰ï¼šç›˜åé“¾è·¯ã€å‘¨æœ«ç»´æŠ¤ã€å¤±è´¥é‡è¯•ç­‰ã€‚"""

import logging
import time
import traceback
from datetime import date, datetime

from app.cache.redis_client import get_redis
from app.cache.tech_cache import refresh_all_tech_cache
from app.config import settings
from app.data.manager import DataManager
from app.data.tushare import TushareClient
from app.database import async_session_factory
from app.scheduler.task_logger import TaskLogger
from app.strategy.factory import StrategyFactory
from app.strategy.pipeline import execute_pipeline

logger = logging.getLogger(__name__)

# å…¨å±€ TaskLogger å®ä¾‹
_task_logger = TaskLogger(async_session_factory)


def _build_manager() -> DataManager:
    """æ„é€  DataManager å®ä¾‹ï¼ˆä½¿ç”¨ TushareClientï¼‰ã€‚"""
    client = TushareClient()
    return DataManager(
        session_factory=async_session_factory,
        clients={"tushare": client},
        primary="tushare",
    )


async def run_post_market_chain(target_date: date | None = None) -> None:
    """ç›˜åé“¾è·¯ï¼šåŒæ­¥é” â†’ è‚¡ç¥¨åˆ—è¡¨ â†’ è¿›åº¦åˆå§‹åŒ– â†’ æ‰¹é‡å¤„ç† â†’ å®Œæ•´æ€§é—¨æ§ â†’ ç­–ç•¥ â†’ é‡Šæ”¾é”ã€‚

    ä½¿ç”¨è¿›åº¦è¡¨é©±åŠ¨ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ã€‚ä»»ä¸€å…³é”®æ­¥éª¤å¤±è´¥åˆ™ä¸­æ–­é“¾è·¯ã€‚

    Args:
        target_date: ç›®æ ‡æ—¥æœŸï¼Œé»˜è®¤ä»Šå¤©
    """
    target = target_date or date.today()
    chain_start = time.monotonic()
    logger.info("===== [ç›˜åé“¾è·¯] å¼€å§‹ï¼š%s =====", target)

    manager = _build_manager()

    # è®°å½•ä»»åŠ¡å¼€å§‹
    log_id = await _task_logger.start("post_market_pipeline", trade_date=target)

    # æ­¥éª¤ 0ï¼šäº¤æ˜“æ—¥å†æ›´æ–°ï¼ˆéå…³é”®ï¼Œå¤±è´¥ä¸é˜»æ–­ï¼‰
    calendar_start = time.monotonic()
    try:
        calendar_result = await manager.sync_trade_calendar()
        logger.info("[äº¤æ˜“æ—¥å†æ›´æ–°] å®Œæˆï¼š%sï¼Œè€—æ—¶ %.2fs", calendar_result, time.monotonic() - calendar_start)
    except Exception:
        logger.warning("[äº¤æ˜“æ—¥å†æ›´æ–°] å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰ï¼Œè€—æ—¶ %.2fs\n%s", time.monotonic() - calendar_start, traceback.format_exc())

    # äº¤æ˜“æ—¥æ ¡éªŒ
    is_trading = await manager.is_trade_day(target)
    if not is_trading:
        logger.info("[ç›˜åé“¾è·¯] éäº¤æ˜“æ—¥ï¼Œè·³è¿‡ï¼š%s", target)
        return

    # è·å–åŒæ­¥é”
    if not await manager.acquire_sync_lock():
        logger.warning("[ç›˜åé“¾è·¯] åŒæ­¥é”è¢«å ç”¨ï¼Œè·³è¿‡ï¼š%s", target)
        return

    try:
        # æ­¥éª¤ 1ï¼šæ›´æ–°è‚¡ç¥¨åˆ—è¡¨
        stock_list_start = time.monotonic()
        try:
            stock_result = await manager.sync_stock_list()
            logger.info("[è‚¡ç¥¨åˆ—è¡¨æ›´æ–°] å®Œæˆï¼š%sï¼Œè€—æ—¶ %.1fs", stock_result, time.monotonic() - stock_list_start)
        except Exception:
            logger.warning("[è‚¡ç¥¨åˆ—è¡¨æ›´æ–°] å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰ï¼Œè€—æ—¶ %.1fs\n%s", time.monotonic() - stock_list_start, traceback.format_exc())

        # æ­¥éª¤ 2ï¼šé‡ç½® stale çŠ¶æ€ + åˆå§‹åŒ–è¿›åº¦è¡¨ + åŒæ­¥é€€å¸‚çŠ¶æ€
        stale_count = await manager.reset_stale_status()
        if stale_count > 0:
            logger.info("[ç›˜åé“¾è·¯] é‡ç½® stale çŠ¶æ€ï¼š%d æ¡", stale_count)

        init_result = await manager.init_sync_progress()
        logger.info("[ç›˜åé“¾è·¯] è¿›åº¦è¡¨åˆå§‹åŒ–ï¼š%s", init_result)

        delisted_result = await manager.sync_delisted_status()
        logger.info("[ç›˜åé“¾è·¯] é€€å¸‚çŠ¶æ€åŒæ­¥ï¼š%s", delisted_result)

        # æ­¥éª¤ 3ï¼šæ‰¹é‡æ•°æ®æ‹‰å– + æŒ‡æ ‡è®¡ç®—ï¼ˆæŒ‰æ—¥æœŸæ‰¹é‡æ‹‰ï¼Œæ¯”é€åªå¿« 100 å€ï¼‰
        step3_start = time.monotonic()
        try:
            sync_result = await manager.sync_daily_by_date([target])
            logger.info(
                "[ç›˜åé“¾è·¯] æ­¥éª¤ 3 å®Œæˆï¼š%sï¼Œè€—æ—¶ %.1fs",
                sync_result, time.monotonic() - step3_start,
            )
        except Exception:
            logger.warning(
                "[ç›˜åé“¾è·¯] æ­¥éª¤ 3 å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰ï¼Œè€—æ—¶ %.1fs\n%s",
                time.monotonic() - step3_start, traceback.format_exc(),
            )

        # æ­¥éª¤ 3.1ï¼šP1 è´¢åŠ¡æ•°æ®åŒæ­¥ï¼ˆæŒ‰å­£åº¦åˆ¤æ–­ï¼Œéå…³é”®ï¼Œå¤±è´¥ä¸é˜»æ–­ï¼‰
        p1_start = time.monotonic()
        try:
            # åˆ¤æ–­å½“å‰å­£åº¦çš„æŠ¥å‘ŠæœŸï¼ˆä¸Šä¸€å­£åº¦æœ«ï¼‰
            # Q1(1-3æœˆ)â†’ä¸Šå¹´12.31, Q2(4-6æœˆ)â†’3.31, Q3(7-9æœˆ)â†’6.30, Q4(10-12æœˆ)â†’9.30
            month = target.month
            year = target.year
            if month <= 3:
                period = f"{year - 1}1231"
            elif month <= 6:
                period = f"{year}0331"
            elif month <= 9:
                period = f"{year}0630"
            else:
                period = f"{year}0930"

            # æ¯å­£åº¦é¦–æœˆæ‰§è¡Œï¼ˆ1/4/7/10æœˆï¼‰ï¼Œé¿å…æ¯å¤©é‡å¤æ‹‰å–
            if month in (1, 4, 7, 10):
                raw_result = await manager.sync_raw_fina(period)
                etl_result = await manager.etl_fina(period)
                logger.info(
                    "[P1è´¢åŠ¡æ•°æ®] å®Œæˆï¼ˆå«è´¢åŠ¡ä¸‰è¡¨ï¼‰ï¼šperiod=%s, raw=%s, etl=%sï¼Œè€—æ—¶ %.1fs",
                    period, raw_result, etl_result, time.monotonic() - p1_start,
                )
            else:
                logger.debug("[P1è´¢åŠ¡æ•°æ®] éå­£åº¦é¦–æœˆï¼Œè·³è¿‡")
        except Exception:
            logger.warning(
                "[P1è´¢åŠ¡æ•°æ®] å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰ï¼Œè€—æ—¶ %.1fs\n%s",
                time.monotonic() - p1_start, traceback.format_exc(),
            )

        # æ­¥éª¤ 3.5ï¼šP2 èµ„é‡‘æµå‘åŒæ­¥ï¼ˆéå…³é”®ï¼Œå¤±è´¥ä¸é˜»æ–­ï¼‰
        p2_start = time.monotonic()
        try:
            p2_result = await manager.sync_raw_tables("p2", target, target, mode="incremental")
            logger.info(
                "[P2èµ„é‡‘æµå‘] å®Œæˆï¼š%sï¼Œè€—æ—¶ %.1fs",
                p2_result, time.monotonic() - p2_start,
            )
        except Exception:
            logger.warning(
                "[P2èµ„é‡‘æµå‘] å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰ï¼Œè€—æ—¶ %.1fs\n%s",
                time.monotonic() - p2_start, traceback.format_exc(),
            )

        # æ­¥éª¤ 3.6ï¼šP3 æŒ‡æ•°æ•°æ®åŒæ­¥ï¼ˆéå…³é”®ï¼Œå¤±è´¥ä¸é˜»æ–­ï¼‰
        p3_start = time.monotonic()
        try:
            p3_result = await manager.sync_raw_tables("p3_daily", target, target, mode="incremental")
            logger.info(
                "[P3æŒ‡æ•°æ•°æ®] å®Œæˆï¼š%sï¼Œè€—æ—¶ %.1fs",
                p3_result, time.monotonic() - p3_start,
            )
        except Exception:
            logger.warning(
                "[P3æŒ‡æ•°æ•°æ®] å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰ï¼Œè€—æ—¶ %.1fs\n%s",
                time.monotonic() - p3_start, traceback.format_exc(),
            )

        # æ­¥éª¤ 3.7ï¼šæ¿å—æ•°æ®åŒæ­¥ï¼ˆéå…³é”®ï¼Œå¤±è´¥ä¸é˜»æ–­ï¼‰
        concept_start = time.monotonic()
        try:
            td_str = target.strftime("%Y%m%d")
            concept_daily = await manager.sync_concept_daily(trade_date=td_str)

            # åŒæ­¥æ¿å—æˆåˆ†è‚¡ï¼ˆæ¯å‘¨ä¸€æ‰§è¡Œï¼Œé¿å…æ¯å¤©é‡å¤æ‹‰å–ï¼‰
            if target.weekday() == 0:  # å‘¨ä¸€
                from sqlalchemy import text as sa_text
                async with async_session_factory() as session:
                    rows = await session.execute(
                        sa_text("SELECT ts_code FROM concept_index")
                    )
                    concept_codes = [r.ts_code for r in rows]
                member_ok, member_fail = 0, 0
                for code in concept_codes:
                    try:
                        await manager.sync_concept_member(code)
                        member_ok += 1
                    except Exception:
                        member_fail += 1
                logger.info("[æ¿å—æˆåˆ†è‚¡] å®Œæˆï¼šæˆåŠŸ %dï¼Œå¤±è´¥ %d", member_ok, member_fail)

            concept_tech = await manager.update_concept_indicators(target)
            logger.info(
                "[æ¿å—æ•°æ®åŒæ­¥] å®Œæˆï¼šdaily=%s, tech=%sï¼Œè€—æ—¶ %.1fs",
                concept_daily, concept_tech, time.monotonic() - concept_start,
            )
        except Exception:
            logger.warning(
                "[æ¿å—æ•°æ®åŒæ­¥] å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰ï¼Œè€—æ—¶ %.1fs\n%s",
                time.monotonic() - concept_start, traceback.format_exc(),
            )

        # æ­¥éª¤ 3.8ï¼šP5 æ ¸å¿ƒæ•°æ®åŒæ­¥ï¼ˆéå…³é”®ï¼Œå¤±è´¥ä¸é˜»æ–­ï¼‰
        p5_start = time.monotonic()
        try:
            p5_result = await manager.sync_p5_core(target)
            logger.info(
                "[P5æ ¸å¿ƒæ•°æ®åŒæ­¥] å®Œæˆï¼š%sï¼Œè€—æ—¶ %.1fs",
                p5_result, time.monotonic() - p5_start,
            )
        except Exception:
            logger.warning(
                "[P5æ ¸å¿ƒæ•°æ®åŒæ­¥] å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰ï¼Œè€—æ—¶ %.1fs\n%s",
                time.monotonic() - p5_start, traceback.format_exc(),
            )

        # æ­¥éª¤ 3.85ï¼šraw è¿½å¹³æ£€æŸ¥ï¼ˆæ‰«æ raw_sync_progressï¼Œè¡¥é½é—æ¼çš„è¡¨ï¼‰
        gap_start = time.monotonic()
        try:
            gap_result = await manager.sync_raw_tables(
                "all", target, target, mode="gap_fill"
            )
            if gap_result:
                gap_filled = sum(
                    1 for v in gap_result.values()
                    if isinstance(v, dict) and v.get("rows", 0) > 0
                )
                logger.info(
                    "[rawè¿½å¹³æ£€æŸ¥] å®Œæˆï¼šæ£€æŸ¥ %d å¼ è¡¨ï¼Œè¡¥é½ %d å¼ ï¼Œè€—æ—¶ %.1fs",
                    len(gap_result), gap_filled, time.monotonic() - gap_start,
                )
        except Exception:
            logger.warning(
                "[rawè¿½å¹³æ£€æŸ¥] å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰ï¼Œè€—æ—¶ %.1fs\n%s",
                time.monotonic() - gap_start, traceback.format_exc(),
            )

        # æ­¥éª¤ 3.9ï¼šæ–°é—»é‡‡é›†ä¸æƒ…æ„Ÿåˆ†æï¼ˆå— news_crawl_enabled æ§åˆ¶ï¼Œå¤±è´¥ä¸é˜»æ–­ï¼‰
        if settings.news_crawl_enabled:
            news_start = time.monotonic()
            try:
                from app.ai.news_analyzer import (
                    NewsSentimentAnalyzer,
                    aggregate_daily_sentiment,
                    save_announcements,
                    save_daily_sentiment,
                )
                from app.data.sources.fetcher import fetch_all_news

                # è·å–æ´»è·ƒè‚¡ç¥¨ä»£ç ï¼ˆå–å‰ 100 åªçƒ­é—¨è‚¡ç¥¨ï¼‰
                from sqlalchemy import text as sa_text
                async with async_session_factory() as session:
                    rows = await session.execute(
                        sa_text(
                            "SELECT ts_code FROM stocks "
                            "WHERE list_status = 'L' "
                            "ORDER BY ts_code LIMIT 100"
                        )
                    )
                    active_codes = [r.ts_code for r in rows]

                if active_codes:
                    # é‡‡é›†æ–°é—»
                    raw_news = await fetch_all_news(active_codes, target)
                    logger.info("[æ–°é—»é‡‡é›†] é‡‡é›†åˆ° %d æ¡æ–°é—»", len(raw_news))

                    # æƒ…æ„Ÿåˆ†æ
                    analyzer = NewsSentimentAnalyzer()
                    analyzed = await analyzer.analyze(raw_news)

                    # ä¿å­˜å…¬å‘Š
                    saved_ann = await save_announcements(analyzed, async_session_factory)

                    # èšåˆå¹¶ä¿å­˜æ¯æ—¥æƒ…æ„Ÿ
                    daily = aggregate_daily_sentiment(analyzed, target)
                    saved_daily = await save_daily_sentiment(daily, async_session_factory)

                    logger.info(
                        "[æ–°é—»èˆ†æƒ…] å®Œæˆï¼šé‡‡é›† %d æ¡ï¼Œä¿å­˜å…¬å‘Š %d æ¡ï¼Œæ¯æ—¥èšåˆ %d æ¡ï¼Œè€—æ—¶ %.1fs",
                        len(raw_news), saved_ann, saved_daily,
                        time.monotonic() - news_start,
                    )
                else:
                    logger.info("[æ–°é—»èˆ†æƒ…] æ— æ´»è·ƒè‚¡ç¥¨ï¼Œè·³è¿‡")
            except Exception:
                logger.warning(
                    "[æ–°é—»èˆ†æƒ…] å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰ï¼Œè€—æ—¶ %.1fs\n%s",
                    time.monotonic() - news_start, traceback.format_exc(),
                )
        else:
            logger.info("[æ–°é—»èˆ†æƒ…] æœªå¯ç”¨ï¼ˆnews_crawl_enabled=falseï¼‰ï¼Œè·³è¿‡")

        # æ­¥éª¤ 4ï¼šç¼“å­˜åˆ·æ–°ï¼ˆéå…³é”®ï¼Œå¤±è´¥ä¸é˜»æ–­ï¼‰
        await cache_refresh_step(target)

        # æ­¥éª¤ 5ï¼šå®Œæ•´æ€§é—¨æ§ â†’ ç­–ç•¥æ‰§è¡Œ/è·³è¿‡
        summary = await manager.get_sync_summary(target)
        completion_rate = summary["completion_rate"]
        threshold = settings.data_completeness_threshold

        if completion_rate >= threshold:
            logger.info(
                "[å®Œæ•´æ€§é—¨æ§] é€šè¿‡ï¼šå®Œæˆç‡ %.1f%% >= é˜ˆå€¼ %.1f%%ï¼Œæ‰§è¡Œç­–ç•¥",
                completion_rate * 100, threshold * 100,
            )
            picks = []
            plans = []
            try:
                picks = await pipeline_step(target)
            except Exception:
                logger.error("[ç›˜åé“¾è·¯] ç­–ç•¥ç®¡é“æ‰§è¡Œå¤±è´¥\n%s", traceback.format_exc())

            # æ­¥éª¤ 5.05ï¼šç”Ÿæˆäº¤æ˜“è®¡åˆ’ï¼ˆéå…³é”®ï¼Œå¤±è´¥ä¸é˜»æ–­ï¼‰
            if picks:
                plan_start = time.monotonic()
                try:
                    from app.strategy.trade_plan import TradePlanGenerator
                    generator = TradePlanGenerator()
                    plans = await generator.generate(async_session_factory, picks, target)
                    logger.info(
                        "[äº¤æ˜“è®¡åˆ’] ç”Ÿæˆå®Œæˆï¼š%d æ¡ï¼Œè€—æ—¶ %.1fs",
                        len(plans), time.monotonic() - plan_start,
                    )
                except Exception:
                    logger.warning(
                        "[äº¤æ˜“è®¡åˆ’] ç”Ÿæˆå¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰ï¼Œè€—æ—¶ %.1fs\n%s",
                        time.monotonic() - plan_start, traceback.format_exc(),
                    )

            # æ­¥éª¤ 5.1ï¼šå›å¡«é€‰è‚¡æ”¶ç›Šç‡ï¼ˆéå…³é”®ï¼Œå¤±è´¥ä¸é˜»æ–­ï¼‰
            returns_start = time.monotonic()
            try:
                returns_result = await manager.update_pick_returns(target)
                logger.info(
                    "[é€‰è‚¡æ”¶ç›Šå›å¡«] å®Œæˆï¼š%sï¼Œè€—æ—¶ %.1fs",
                    returns_result, time.monotonic() - returns_start,
                )
            except Exception:
                logger.warning(
                    "[é€‰è‚¡æ”¶ç›Šå›å¡«] å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰ï¼Œè€—æ—¶ %.1fs\n%s",
                    time.monotonic() - returns_start, traceback.format_exc(),
                )

            # æ­¥éª¤ 5.2ï¼šè®¡ç®—å‘½ä¸­ç‡ç»Ÿè®¡ï¼ˆéå…³é”®ï¼Œå¤±è´¥ä¸é˜»æ–­ï¼‰
            stats_start = time.monotonic()
            try:
                stats_result = await manager.compute_hit_stats(target)
                logger.info(
                    "[å‘½ä¸­ç‡ç»Ÿè®¡] å®Œæˆï¼š%sï¼Œè€—æ—¶ %.1fs",
                    stats_result, time.monotonic() - stats_start,
                )
            except Exception:
                logger.warning(
                    "[å‘½ä¸­ç‡ç»Ÿè®¡] å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰ï¼Œè€—æ—¶ %.1fs\n%s",
                    time.monotonic() - stats_start, traceback.format_exc(),
                )

            # æ­¥éª¤ 5.5ï¼šAI åˆ†æï¼ˆéå…³é”®ï¼Œå¤±è´¥ä¸é˜»æ–­ï¼‰
            if picks:
                ai_start = time.monotonic()
                try:
                    from app.ai.manager import get_ai_manager
                    ai_manager = get_ai_manager()
                    if ai_manager.is_enabled:
                        # å– Top 30 å€™é€‰è‚¡è¿›è¡Œ AI åˆ†æ
                        top_picks = picks[:30]
                        # æ„å»º market_dataï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨ pick è‡ªèº«æ•°æ®ï¼‰
                        market_data = {
                            p.ts_code: {"close": p.close, "pct_chg": p.pct_chg}
                            for p in top_picks
                        }
                        analyzed = await ai_manager.analyze(top_picks, market_data, target)
                        # è·å– token ç”¨é‡å¹¶æŒä¹…åŒ–
                        token_usage = ai_manager._get_client().get_last_usage() if ai_manager._client else None
                        saved = await ai_manager.save_results(
                            analyzed, target, async_session_factory, token_usage
                        )
                        logger.info(
                            "[AIåˆ†æ] å®Œæˆï¼šåˆ†æ %d åªï¼ŒæŒä¹…åŒ– %d æ¡ï¼Œè€—æ—¶ %.1fs",
                            len(top_picks), saved, time.monotonic() - ai_start,
                        )
                    else:
                        logger.info("[AIåˆ†æ] æœªå¯ç”¨ï¼Œè·³è¿‡")
                except Exception:
                    logger.warning(
                        "[AIåˆ†æ] å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰ï¼Œè€—æ—¶ %.1fs\n%s",
                        time.monotonic() - ai_start, traceback.format_exc(),
                    )
        else:
            logger.warning(
                "[å®Œæ•´æ€§é—¨æ§] æœªé€šè¿‡ï¼šå®Œæˆç‡ %.1f%% < é˜ˆå€¼ %.1f%%ï¼Œè·³è¿‡ç­–ç•¥ï¼ˆæ€» %dï¼Œå®Œæˆ %dï¼Œå¤±è´¥ %dï¼‰",
                completion_rate * 100, threshold * 100,
                summary["total"], summary["data_done"], summary["failed"],
            )

        # æ­¥éª¤ 6ï¼šå®Œæ•´æ€§å‘Šè­¦ï¼ˆè¶…è¿‡æˆªæ­¢æ—¶é—´ä¸”æœ‰å¤±è´¥è®°å½•ï¼‰
        _check_completeness_deadline(summary, target)

    finally:
        await manager.release_sync_lock()

    elapsed = time.monotonic() - chain_start
    elapsed_minutes = int(elapsed / 60)
    elapsed_seconds = int(elapsed % 60)
    logger.info(
        "===== [ç›˜åé“¾è·¯] å®Œæˆï¼š%sï¼Œæ€»è€—æ—¶ %dåˆ†%dç§’ (%.1fs) =====",
        target, elapsed_minutes, elapsed_seconds, elapsed,
    )

    # æ­¥éª¤ 7ï¼šå‘é€ Telegram é€šçŸ¥ï¼ˆå…¨é¢ç›˜ååˆ†ææŠ¥å‘Šï¼‰
    try:
        from app.notification import NotificationManager, NotificationLevel
        notifier = NotificationManager()
        pick_count = len(picks) if picks else 0
        plan_count = len(plans) if plans else 0

        msg_lines = [
            f"ğŸ“… ç›˜ååˆ†ææŠ¥å‘Š â€” {target}",
            f"{'â”€' * 28}",
            "",
            "ğŸ“Š æ‰§è¡Œæ¦‚å†µ",
            f"  â± æ€»è€—æ—¶: {elapsed_minutes}åˆ†{elapsed_seconds}ç§’",
            f"  ğŸ“ˆ æ•°æ®åŒæ­¥: {summary.get('data_done', 'N/A')} åªè‚¡ç¥¨",
            f"  ğŸ¯ é€‰è‚¡å‘½ä¸­: {pick_count} æ¡",
            f"  ğŸ“‹ äº¤æ˜“è®¡åˆ’: {plan_count} æ¡",
        ]

        # ç­–ç•¥åˆ†å¸ƒç»Ÿè®¡
        if picks:
            from collections import Counter
            strategy_counter = Counter()
            for p in picks:
                for s in p.matched_strategies:
                    strategy_counter[s] += 1
            msg_lines.append("")
            msg_lines.append(f"ğŸ“Œ ç­–ç•¥åˆ†å¸ƒï¼ˆå…± {len(strategy_counter)} ä¸ªç­–ç•¥å‘½ä¸­ï¼‰")
            for sname, cnt in strategy_counter.most_common(10):
                msg_lines.append(f"  â€¢ {sname}: {cnt} åª")
            if len(strategy_counter) > 10:
                msg_lines.append(f"  â€¦ åŠå…¶ä»– {len(strategy_counter) - 10} ä¸ªç­–ç•¥")

        # æ¶¨è·Œå¹…åˆ†å¸ƒ
        if picks:
            up_count = sum(1 for p in picks if p.pct_chg > 0)
            down_count = sum(1 for p in picks if p.pct_chg < 0)
            flat_count = sum(1 for p in picks if p.pct_chg == 0)
            avg_chg = sum(p.pct_chg for p in picks) / len(picks)
            msg_lines.append("")
            msg_lines.append("ğŸ“‰ é€‰è‚¡æ± æ¶¨è·Œåˆ†å¸ƒ")
            msg_lines.append(f"  ğŸ”´ ä¸Šæ¶¨: {up_count}  ğŸŸ¢ ä¸‹è·Œ: {down_count}  âšª å¹³ç›˜: {flat_count}")
            msg_lines.append(f"  ğŸ“Š å¹³å‡æ¶¨è·Œå¹…: {avg_chg:+.2f}%")

        # Top 10 å€™é€‰
        if picks:
            msg_lines.append("")
            msg_lines.append("ğŸ† Top 10 å€™é€‰")
            for i, p in enumerate(picks[:10], 1):
                name = getattr(p, 'name', '') or p.ts_code
                chg_str = f"{p.pct_chg:+.2f}%" if p.pct_chg else ""
                strats = ",".join(p.matched_strategies[:3])
                if len(p.matched_strategies) > 3:
                    strats += f"+{len(p.matched_strategies)-3}"
                msg_lines.append(
                    f"  {i}. {p.ts_code} {name}"
                    f"\n     æ”¶ç›˜:{p.close} æ¶¨è·Œ:{chg_str} å¾—åˆ†:{p.weighted_score:.2f}"
                    f"\n     ç­–ç•¥:{strats}"
                )

        # äº¤æ˜“è®¡åˆ’æ‘˜è¦
        if plans:
            msg_lines.append("")
            msg_lines.append(f"ğŸ“‹ äº¤æ˜“è®¡åˆ’ Top 10ï¼ˆå…± {plan_count} æ¡ï¼‰")
            # plans æ˜¯ dict åˆ—è¡¨
            for i, pl in enumerate(plans[:10], 1):
                code = pl.get('ts_code', '')
                trigger = pl.get('trigger_type', '')
                tp = pl.get('trigger_price')
                sl = pl.get('stop_loss')
                tkp = pl.get('take_profit')
                strategy = pl.get('source_strategy', '')
                tp_str = f"è§¦å‘:{tp}" if tp else ""
                sl_str = f"æ­¢æŸ:{sl}" if sl else ""
                tkp_str = f"æ­¢ç›ˆ:{tkp}" if tkp else ""
                prices = " | ".join(filter(None, [tp_str, sl_str, tkp_str]))
                msg_lines.append(
                    f"  {i}. {code} [{trigger}]"
                    f"\n     {prices}"
                    f"\n     æ¥æº:{strategy}"
                )

        msg_lines.append("")
        msg_lines.append(f"{'â”€' * 28}")
        msg_lines.append("ğŸ¤– é€‰è‚¡ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ")

        await notifier.send(
            NotificationLevel.INFO,
            f"âœ… ç›˜åé“¾è·¯å®Œæˆ â€” {target}",
            "\n".join(msg_lines),
        )
        logger.info("[Telegramé€šçŸ¥] å·²å‘é€")
    except Exception:
        logger.warning("[Telegramé€šçŸ¥] å‘é€å¤±è´¥\n%s", traceback.format_exc())

    # è®°å½•ä»»åŠ¡å®Œæˆ
    await _task_logger.finish(
        log_id,
        status="success",
        result_summary={"elapsed_seconds": round(elapsed, 1)},
    )


def _check_completeness_deadline(summary: dict, target_date: date) -> None:
    """æ£€æŸ¥å®Œæ•´æ€§å‘Šè­¦ï¼šè¶…è¿‡æˆªæ­¢æ—¶é—´ä¸”æœ‰å¤±è´¥è®°å½•æ—¶å‘å‡ºå‘Šè­¦ã€‚

    Args:
        summary: get_sync_summary() è¿”å›çš„æ‘˜è¦
        target_date: ç›®æ ‡æ—¥æœŸ
    """
    if summary["failed"] == 0:
        return

    deadline_str = settings.pipeline_completeness_deadline
    deadline_time = datetime.strptime(deadline_str, "%H:%M").time()
    current_time = datetime.now().time()

    if current_time >= deadline_time:
        logger.warning(
            "[å®Œæ•´æ€§å‘Šè­¦] %s è¶…è¿‡æˆªæ­¢æ—¶é—´ %sï¼Œä»æœ‰ %d åªè‚¡ç¥¨å¤±è´¥ï¼ˆå®Œæˆç‡ %.1f%%ï¼‰",
            target_date, deadline_str, summary["failed"],
            summary["completion_rate"] * 100,
        )


async def cache_refresh_step(target_date: date) -> None:
    """åˆ·æ–°æŠ€æœ¯æŒ‡æ ‡ç¼“å­˜ï¼ˆéå…³é”®æ­¥éª¤ï¼Œå¤±è´¥ä¸é˜»æ–­é“¾è·¯ï¼‰ã€‚

    Args:
        target_date: ç›®æ ‡æ—¥æœŸ
    """
    redis = get_redis()
    if redis is None:
        logger.warning("[ç¼“å­˜åˆ·æ–°] Redis ä¸å¯ç”¨ï¼Œè·³è¿‡")
        return

    step_start = time.monotonic()
    logger.info("[ç¼“å­˜åˆ·æ–°] å¼€å§‹ï¼š%s", target_date)

    try:
        count = await refresh_all_tech_cache(redis, async_session_factory)
        elapsed = time.monotonic() - step_start
        logger.info("[ç¼“å­˜åˆ·æ–°] å®Œæˆï¼š%d åªè‚¡ç¥¨ï¼Œæ€»è€—æ—¶ %.1fs", count, elapsed)
    except Exception as e:
        elapsed = time.monotonic() - step_start
        logger.warning("[ç¼“å­˜åˆ·æ–°] å¤±è´¥ï¼ˆè€—æ—¶ %.1fsï¼‰ï¼Œç­–ç•¥ç®¡é“å°†å›æºæ•°æ®åº“ï¼š%s", elapsed, e)


async def pipeline_step(target_date: date) -> list:
    """æ‰§è¡Œç­–ç•¥ç®¡é“ï¼šä»…æ‰§è¡Œç”¨æˆ·å¯ç”¨çš„ç­–ç•¥ï¼ˆæ³¨å†Œåˆ¶ï¼‰ã€‚

    ä» strategies è¡¨è¯»å– is_enabled=True çš„ç­–ç•¥åŠå…¶è‡ªå®šä¹‰å‚æ•°ï¼Œ
    ä¼ é€’ç»™ execute_pipeline æ‰§è¡Œã€‚

    Args:
        target_date: ç›®æ ‡æ—¥æœŸ

    Returns:
        å€™é€‰è‚¡ç¥¨åˆ—è¡¨ï¼ˆStockPickï¼‰
    """
    import json
    from sqlalchemy import text as sa_text

    step_start = time.monotonic()
    logger.info("[ç­–ç•¥ç®¡é“] å¼€å§‹ï¼š%s", target_date)

    # ä» strategies è¡¨æŸ¥è¯¢å¯ç”¨çš„ç­–ç•¥
    async with async_session_factory() as session:
        result = await session.execute(
            sa_text("SELECT name, params FROM strategies WHERE is_enabled = true")
        )
        rows = result.fetchall()

    if not rows:
        logger.warning("[ç­–ç•¥ç®¡é“] æ²¡æœ‰å¯ç”¨çš„ç­–ç•¥ï¼Œè·³è¿‡æ‰§è¡Œ")
        return []

    strategy_names = []
    strategy_params: dict[str, dict] = {}
    for name, params_str in rows:
        # æ ¡éªŒç­–ç•¥åœ¨å†…å­˜æ³¨å†Œè¡¨ä¸­å­˜åœ¨
        try:
            StrategyFactory.get_meta(name)
        except KeyError:
            logger.warning("[ç­–ç•¥ç®¡é“] ç­–ç•¥ %s å·²å¯ç”¨ä½†æœªæ³¨å†Œï¼Œè·³è¿‡", name)
            continue
        strategy_names.append(name)
        # è§£æè‡ªå®šä¹‰å‚æ•°
        if params_str:
            try:
                p = json.loads(params_str) if isinstance(params_str, str) else params_str
                if p:
                    strategy_params[name] = p
            except (json.JSONDecodeError, TypeError):
                pass

    if not strategy_names:
        logger.warning("[ç­–ç•¥ç®¡é“] æ²¡æœ‰æœ‰æ•ˆçš„å¯ç”¨ç­–ç•¥ï¼Œè·³è¿‡æ‰§è¡Œ")
        return []

    logger.info("[ç­–ç•¥ç®¡é“] å¯ç”¨ç­–ç•¥ %d ä¸ªï¼š%s", len(strategy_names), strategy_names)

    result = await execute_pipeline(
        session_factory=async_session_factory,
        strategy_names=strategy_names,
        target_date=target_date,
        top_n=50,
        strategy_params=strategy_params or None,
    )

    elapsed = int(time.monotonic() - step_start)
    logger.info(
        "[ç­–ç•¥ç®¡é“] å®Œæˆï¼šç­›é€‰å‡º %d åªï¼Œè€—æ—¶ %dms",
        len(result.picks), result.elapsed_ms,
    )
    return result.picks


async def sync_stock_list_job() -> None:
    """å‘¨æœ«è‚¡ç¥¨åˆ—è¡¨å…¨é‡åŒæ­¥ã€‚"""
    manager = _build_manager()

    # æ­¥éª¤ 1ï¼šæ›´æ–°äº¤æ˜“æ—¥å†
    logger.info("[äº¤æ˜“æ—¥å†æ›´æ–°] å¼€å§‹")
    try:
        calendar_result = await manager.sync_trade_calendar()
        logger.info("[äº¤æ˜“æ—¥å†æ›´æ–°] å®Œæˆï¼š%s", calendar_result)
    except Exception:
        logger.error("[äº¤æ˜“æ—¥å†æ›´æ–°] å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œè‚¡ç¥¨åˆ—è¡¨åŒæ­¥\n%s", traceback.format_exc())

    # æ­¥éª¤ 2ï¼šåŒæ­¥è‚¡ç¥¨åˆ—è¡¨
    logger.info("[è‚¡ç¥¨åˆ—è¡¨åŒæ­¥] å¼€å§‹")
    result = await manager.sync_stock_list()
    logger.info("[è‚¡ç¥¨åˆ—è¡¨åŒæ­¥] å®Œæˆï¼š%s", result)


async def retry_failed_stocks_job() -> None:
    """å®šæ—¶é‡è¯•å¤±è´¥è‚¡ç¥¨ï¼šè·å–åŒæ­¥é” â†’ æŸ¥è¯¢å¤±è´¥è‚¡ç¥¨ â†’ é€åªé‡è¯• â†’ æ£€æŸ¥å®Œæ•´æ€§ â†’ é‡Šæ”¾é”ã€‚

    æ¯æ¬¡é‡è¯• retry_count+1ï¼Œè¶…è¿‡ max_retries çš„è‚¡ç¥¨è®°å½• WARNING ä¸å†é‡è¯•ã€‚
    é‡è¯•å®Œæˆåæ£€æŸ¥å®Œæ•´æ€§ï¼Œè¾¾åˆ°é˜ˆå€¼åˆ™è¡¥è·‘ç­–ç•¥ã€‚
    """
    target = date.today()
    start = time.monotonic()
    logger.info("[å¤±è´¥é‡è¯•] å¼€å§‹ï¼š%s", target)

    manager = _build_manager()
    max_retries = settings.batch_sync_max_retries

    # è·å–åŒæ­¥é”
    if not await manager.acquire_sync_lock():
        logger.warning("[å¤±è´¥é‡è¯•] åŒæ­¥é”è¢«å ç”¨ï¼Œè·³è¿‡")
        return

    try:
        # æŸ¥è¯¢å¯é‡è¯•çš„å¤±è´¥è‚¡ç¥¨
        failed_stocks = await manager.get_failed_stocks(max_retries)
        if not failed_stocks:
            logger.info("[å¤±è´¥é‡è¯•] æ— éœ€é‡è¯•çš„å¤±è´¥è‚¡ç¥¨")
            return

        logger.info("[å¤±è´¥é‡è¯•] å¾…é‡è¯•è‚¡ç¥¨ %d åª", len(failed_stocks))

        # æ£€æŸ¥è¶…è¿‡é‡è¯•ä¸Šé™çš„è‚¡ç¥¨
        all_failed = await manager.get_failed_stocks(max_retries=999999)
        exceeded = [s for s in all_failed if s["retry_count"] >= max_retries]
        if exceeded:
            codes = [s["ts_code"] for s in exceeded[:10]]
            logger.warning(
                "[å¤±è´¥é‡è¯•] %d åªè‚¡ç¥¨è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•° %dï¼Œä¸å†è‡ªåŠ¨é‡è¯•ï¼š%s%s",
                len(exceeded), max_retries, codes,
                "..." if len(exceeded) > 10 else "",
            )

        # é€’å¢ retry_count å¹¶é‡ç½®çŠ¶æ€ä¸º idleï¼Œç„¶åé‡æ–°å¤„ç†
        from sqlalchemy import update as sa_update
        from app.models.market import StockSyncProgress

        success_count = 0
        fail_count = 0
        for stock in failed_stocks:
            ts_code = stock["ts_code"]
            try:
                # é€’å¢ retry_count
                async with manager.session_factory() as session:
                    await session.execute(
                        sa_update(StockSyncProgress)
                        .where(StockSyncProgress.ts_code == ts_code)
                        .values(retry_count=StockSyncProgress.retry_count + 1, status="idle")
                    )
                    await session.commit()

                # ä» data_date æ¢å¤åŒæ­¥
                await manager.process_single_stock(ts_code, target)
                success_count += 1
            except Exception as e:
                fail_count += 1
                logger.error("[å¤±è´¥é‡è¯•] %s é‡è¯•å¤±è´¥ï¼š%s", ts_code, e)

        elapsed = time.monotonic() - start
        logger.info(
            "[å¤±è´¥é‡è¯•] å®Œæˆï¼šæˆåŠŸ %dï¼Œå¤±è´¥ %dï¼Œè€—æ—¶ %.1fs",
            success_count, fail_count, elapsed,
        )

        # é‡è¯•åæ£€æŸ¥å®Œæ•´æ€§ï¼Œè¾¾åˆ°é˜ˆå€¼åˆ™è¡¥è·‘ç­–ç•¥
        summary = await manager.get_sync_summary(target)
        completion_rate = summary["completion_rate"]
        threshold = settings.data_completeness_threshold

        if completion_rate >= threshold:
            logger.info(
                "[å¤±è´¥é‡è¯•] å®Œæˆç‡ %.1f%% >= é˜ˆå€¼ %.1f%%ï¼Œè¡¥è·‘ç­–ç•¥",
                completion_rate * 100, threshold * 100,
            )
            try:
                await pipeline_step(target)
            except Exception:
                logger.error("[å¤±è´¥é‡è¯•] è¡¥è·‘ç­–ç•¥å¤±è´¥\n%s", traceback.format_exc())
        else:
            logger.info(
                "[å¤±è´¥é‡è¯•] å®Œæˆç‡ %.1f%% < é˜ˆå€¼ %.1f%%ï¼Œè·³è¿‡ç­–ç•¥",
                completion_rate * 100, threshold * 100,
            )

    finally:
        await manager.release_sync_lock()
